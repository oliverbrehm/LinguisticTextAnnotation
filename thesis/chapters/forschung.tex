% !TEX root = ../ausarbeitung.tex

\chapter{Stand der Forschung}

Die Motivation der Arbeit beruht auf Erkenntnissen zu Behandlungsformen für Lese- Rechtschreibschwäche (LRS). Es wird untersucht wie gut der Prozess des Erzeugens von Übungstexten automatisiert werden kann. Hierfür war das Ziel bei der Entwicklung der App eine möglichst gute User Experience zu bieten. Die Zielgruppe, vorwiegend bestehend aus Lerntherapeuten, Nachhilfelehrern und Eltern betroffener Kinder soll das Programm möglichst intuitiv bedienen können.\\
Im Folgenden werden daher erst die Grundlagen zu LRS erklärt und Ansätze zur Digitalisierung von Übungstexten, z.B. auch ähnliche, bereits vorhandene Software, beschrieben. Danach werden die computerlinguistischen technischen Grundlagen erklärt, die in der Entwicklung der Applikation benötigt werden.

\section{Lese- Rechtschreibschwäche}

Ein Ausgangspunkt zur Feststellung von LRS bietet die Lesekompetenz. Eine Definition aus der PISA Studie \tocite{PISA, SK s51} untergliedert die Lesekompetenz in die vier Teilbereiche \textit{Kognitive Grundfähigkeit}, \textit{Dekodierfähigkeit}, \textit{Lernstrategiewissen} und \textit{Leseinteresse}.\\
Der Teilbereich, zu dessen Verbesserung diese Arbeit einen Beitrag leisten kann ist die Dekodierfähigkeit. Diese Stellt die Kompetenz dar, die Bedeutung von Wörtern, Sätzen und Texten zu erfassen und zu verstehen. Die Unterteilung in dei Elemente Wort, Satz und Text stellt eine Dekodierungsfähigkeit auf verschiedenen Ebenen dar, die sich gegenseitig bedingen. Das Wort ist hier die Basisebene, so kann ein Text nur verstanden werden, wenn die Bedeutung der einzelnen Sätze erfasst wurde und diese wiederum wird nur erkannt, wenn eine Ausreichende Dekodierfähigkeit für Wörter vorhanden ist.\\
Somit ist eine Vorraussetzung für die Verbesserung des Textverständnis, dass das Erkennen des Basiselements Wort beherrscht wird.

\subsection{Dekodierung des Wortbildes}

Es ist bekannt, dass Kinder der Entwicklung des Lesens und des Schreibens verschiedene Phasen durchlaufen. \tocite{quelle hier sehr weit weg (steinbrink), vielleicht frueher bringen, weitere quelle} So gibt in jedem Fall eine alphabetische und eine orthographische Phase. Beim Lesen wird in der alphabetischen Phase ein Wort Buchstabe für Buchstabe dekodiert, \textit{Grapheme} (die kleinste Einheit in der Schriftsprache, in deutsch Buchstaben) werden einzeln in \textit{Phoneme} (Laute, die kleinste Einheit der gesprochenen Sprache) umgewandelt, was aber bei nicht lautgetreuen Wörtern (Wörtern, in denen nicht alle Grapheme korrekt in die zugehörigen Phoneme übersetzen lassen) nicht gelingt.\\
Daher wird später in der orthographischen Phase eine Strategie benutzt, die sich an größeren Bestandteilen orientiert. Wörter oder Wortteile werden hier aus dem Langzeitgedächtnis abgerufen, was bei richtig gelernten Silben und Wörtern zu korrekter Aussprache führt. \cite{Steinbrink2014}\\
Einige schulische Ansätze fördern das orthographische Lernen mit einer gezielten Hervorhebung von Silben. \tocite{ABC der Tiere, Silbenfibeln}. \todo{mehr dekodierung stuff} Viele Fördermaterialien sind erhältlich, die Silben beispielsweise farblich hervorheben und damit guten Lernerfolg erzielen. \tocite{ABC}
Weiterführende Arbeiten legen aber auch nahe, dass der Sprachrhythmus eine zentrale Rollen sowohl beim Lesen als auch beim Schreiben spielt. So hat ein Wort in der deutschen Sprache eine oder mehrere Betonungen. Betonte Silben werden, im Gegensatz zu den unbetonten Silben lauter und länger gesprochen. Es wurde gezeigt, dass gezieltes Training des Sprachrhythmus in Verbindung mit der orthographischen Repräsentation eines Wortes die Dekodierfähigkeit steigern kann. \cite{Brandelik2014}
\todo{more betonung stuff}

warum ist zeicheinabstand wichtig? mehr zu font, textgröße, wortabstand... -> Katharina?

\subsection{Therapiemethoden}

Wie kann LRS sonst noch behandelt werden?\\
Methoden zum Lesetraining\\
Farbliche Hervorhebung, Silbenmethode ABC der Tiere, andere quellen
Literatur zu übungen\\

\subsection{Digitale Ansätze}
Vorhandene Methoden\\
ABC der Tiere software, CELESCO ( https://www.celeco.de/?page=LRS\_Texte\_mit\_Silbenboegen\_drucken)
Digitale Tools\\

vorhandene tools vorstellen, Literatur zu Tools, zeigen, wie diese Ideen weitergeführt werden können, übergang zur notwendigkeit des eigenen tools\\

\section{Computerlinguistische Grundlagen}

Um das Ziel einer automatischen Analyse und Annotation beliebiger Texte zu realisieren, werden computerlinguistisch vor Allem die folgenden zwei Schritte untersucht:
\begin{enumerate}
	\item Linguistische Analyse (Zerlegung des Texts in Wörter)
	\item Bestimmung von Silbentrennung und Betonungsmuster
\end{enumerate}

\subsection{Linguistische Analyse}
Da die vom Programm zu verarbeitende Eingabe ein Text ist, also eine beliebige Folge von alphanumerischen Zeichen, sowie Leerraumzeichen und Interpunktionen, muss diese Eingabe zunächst korrekt in Wörter zerteilt werden. Dieser Schritt wird \textbf{Tokenisierung} genannt. Zusätzlich werden auch von einem \textbf{Tagger} jedem Wort weitere Informationen, wie z.B. die Wortart (\textit{Part-of-Speech/POS-Tagger}) hinzugefügt. Diese werden üblicherweise mithilfe eines entsprechenden Lexikons nachgeschlagen. \cite{Carstensen2009}\todo{2. auflage 2014, S. 218-220, 224-226} Der für dieses Projekt verwendete Parser wird in Abschnitt \toref{spacy} näher beschrieben. Beim Parsen des Texts können folgende wichtige Informationen extrahiert werden:

\begin{itemize}
	\item \textbf{Textstruktur}: Zerlegung des Texts in Tokens, dadurch wird bestimmt, ob es sich bei den Zeichenfolgen um Wörter oder andere Strukturen wie Leerraum oder Interpunktionen handelt
	
	\item \textbf{Wortart}: Die Werte der Wortart- (\textit{Part-of-Speech-}) Tags geben an um was für ein Wort es sich handelt (mit dem \textit{spacy} Parser z.B. \textit{NOM} für Nomen,\textit{PROPN} für Eigennamen, \textit{DET} für Artikel etc.)
	
	\item  \textbf{Lemma}: Gibt die Grundform eines Wortes an, welche je nach Wortart unterschiedliche Ausprägungen nehmen kann. Bei Verben steht hier z.B. der Infinitiv, bei Pronomen die erste Person Singular etc. \todo{hier Lemma für alle Wortarten beschreiben?}
\end{itemize}

\subsection{Datenbanken zur Wortbetoung}
Liegt der Text als Liste von Wörtern (Tokens) vor, kann auf dieser Ebene weitergearbeitet werden. Für die Annotation des Textes muss eine Repräsentation in einzelnen Silben vorliegen. Zusätzlich wird für die Hervorhebung des Sprachrhythmus die betonte Silbe im Wort gesucht. Es ist durchaus möglich, dass in einem Wort mehrere Betonungen vorkommen (gerade bei Wortkompositionen, die im Deutschen häufig zu finden sind, z.B. \textit{heraus+kommen}). Dies wird hier vernachlässigt, es wird nur eine Hauptbetonung, die erste vorkommende Betonung im Wort gesucht.\\
Informationen zur Worttrennung in Silben und zur Wortbetonung lassen sich in diversen Lexika finden. \todo{Welche?} Für die weitere Verwendung in der zu entwickelnden Applikation wurde als Grundlage das Lexikon CELEX2 gewählt. Dabei handelt es sich um ein umfangreiches Sprachlexikon für die Sprachen Englisch, Deutsch und Niederländisch. Verarbeitet wurden hier zunächst nur die deutsche Sprache. Neben der gesuchten Worttrennung und -betonung enthält es auch weitere linguistische Informationen wie Wortart und Grundformen sowie phonologische und orthographische Annotationen \tocite{CELEX Publikation} .
\todo{Alternativen, welche? könnten zusätzlich oder anstelle des CELEX verwendet werden}\\
Weiterführend wurden auch Text-To-Speech Systeme \todo{beschreiben} untersucht. Diese müssen um zu funktionieren zwangsweise eine phonologische Analyse durchführen, also auch die Wortbetonung bestimmen. Beispielsweise liefern die Systeme MARY TTS\tocite{MARY, woher?} oder BAS\tocite{BAS} auch textuellen annotiere Formen ihrer phonologischen Analyse, die auch als Grundlage zur Extraktion von Worttrennung und Betonung dienen können.\\

Linguistische Datenbanken sind aufgrund des Umfangs und der Flexibilität von Sprachgrammatiken niemals vollständig\tocite{lexika, datenbanken}. Es wurde daher ein Weg gesucht die Datenbank auf unkomplizierte Weise erweiterbar zu machen. Dies kann mit einem geeignete User-Interface durch Experten erfolgen, die unbekannte Einträge ergänzen und hinzufügen. Einige Arbeiten hatten bereits Erfolg damit, solche Aufgaben effektiv auch auf Nutzer zu verteilen, die keine Experten für den jeweiligen Anwendungsbereich sind. Da die zu entwickelte Applikation auch eine Nutzerverwaltungssystem beinhaltet, wurde untersucht ob Datenbankeinträge mithilfe mehrerer Nicht-Experten Nutzer per einfachem Mehrheitsentscheid verifiziert werden können. Weiterführend könnten auf Plattformen wie Amazon Mechanical Turc oder CrowdFlower beispielsweise leicht Arbeiter anonym engagiert werden, die solche Aufgaben erledigen\cite{Snow2008}. Diese Plattformen wurden beispielsweise von Zaidan und Callison-Burch (für automatisierte Übersetzungen)\cite{Zaidan2011} oder De Kuthy, Ziai und Meurers\cite{Meurers2015} (für Fokusannotation) benutzt, um computerlinguistische Aufgaben zu erledigen.